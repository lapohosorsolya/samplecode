#!/usr/bin/env python3
from math import ceil
import os, sys, argparse
import numpy as np
import pandas as pd
import scanpy as sc
import anndata
import time
from tqdm import tqdm
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from scgo import ldmethods


'''
Trains and tests an iterative random forest classifier to identify intact or broken single cells in scRNA-seq data.
'''


normal = '\033[0m'
blue = '\033[94m' 
red = '\033[93m'


if __name__ == '__main__':

    first_tic = time.process_time()
    now = datetime.now()


    ###### PARSE ARGS ######


    parser = argparse.ArgumentParser(description = 'Use this program to run the iterative random forest classifier (IRFC.py) after makefeatures.py')
    parser.add_argument('workdir', help = 'full path to directory where output files will be written (should contain the output of makefeatures.py)', type = str)
    parser.add_argument('--test', help = 'whether to split the data into training and testing sets (if True, the anndata object must have a true_annotation bool column in obs)', default = False, type = bool)
    parser.add_argument('--nfeatures', help = 'max number of features to use when training a random forest', default = 500, type = int)
    parser.add_argument('--njobs', help = 'number of threads to use', default = 2, type = int)
    parser.add_argument('--rounds', help = 'number of times to iterate through each feature', default = 5, type = int)
    parser.add_argument('--minsplit', help = 'min_samples_split parameter of the Random Forest Classifier (scikit-learn)', default = 0.02, type = float)
    parser.add_argument('--batchremove', help = 'whether batch effect removal should be done (using ComBat built into scanpy)', default = False, type = bool)
    args = parser.parse_args()
    
    work_dir = args.workdir
    split_data = args.test
    n_features = args.nfeatures
    n_jobs = args.njobs
    rounds = args.rounds
    rfc_minsplit = args.minsplit
    batch = args.batchremove


    ###### READ INPUTS ######

    param_file = work_dir + 'params.tsv'
    data_file = work_dir + 'intermediate_data.h5ad'
    feature_dict_file = work_dir + 'feature_dict.tsv'
    feature_list_file = work_dir + 'ranked_feature_list.txt'

    # get relevant information from the params.tsv file generated by makefeatures.py
    if not os.path.isfile(param_file):
        print(red + 'File does not exist: {}\n'.format(param_file) + normal)
        sys.exit(2)
    else:
        params = pd.read_csv(param_file, sep = '\t', header = 0)
        param_dict = params.set_index('var')['val'].to_dict()

    # read anndata file (intermediate generated by makefeatures.py)
    if not os.path.isfile(data_file):
        print(red + 'File does not exist: {}\n'.format(data_file) + normal)
        sys.exit(2)
    else:
        origdata = sc.read_h5ad(data_file)

    # read feature dict
    if not os.path.isfile(feature_dict_file):
        print(red + 'File does not exist: {}\n'.format(feature_dict_file) + normal)
        sys.exit(2)
    else:
        print(blue + 'Reading feature dict from {}'.format(feature_dict_file) + normal)
        feature_dict = {}
        feature_df = pd.read_csv(feature_dict_file, sep = '\t')
        for index, row in feature_df.iterrows():
            feature_dict[row['feature']] = row['genes'].split(',')

    # read feature list
    if not os.path.isfile(feature_list_file):
        print(red + 'File does not exist: {}\n'.format(feature_list_file) + normal)
        sys.exit(2)
    else:
        print(blue + 'Reading feature list from {}'.format(feature_list_file) + normal)
        with open(feature_list_file) as f:
            feature_list = [ line.rstrip() for line in f ]
    

    ###### ORGANIZE DATA ######


    # split data to train and test if needed
    if split_data == True:
        if 'true_annotation' not in origdata.obs.columns:
            origdata.obs['true_annotation'] = (origdata.obs['Label'] == 'U')
        print('Splitting data into train and test set...')
        X_train, X_test, y_train, y_test = train_test_split(origdata.to_df(), origdata.obs.true_annotation, train_size = 0.8, random_state = 49)
        mtx = X_train.values
        obs = origdata.obs.loc[y_train.index]
        var = origdata.var
        traindata = anndata.AnnData(X = mtx, obs = obs, var = var)
        mtx_test = X_test.values
        obs_test = origdata.obs.loc[y_test.index]
        var_test = origdata.var
        testdata = anndata.AnnData(X = mtx_test, obs = obs_test, var = var_test)
    else:
        print('Using full dataset for feature selection...')
        traindata = origdata
    
    # save stuff for run report
    single_cells = len(traindata.obs)
    available_features = len(feature_list)
    feature_data = param_dict['feature_types'].split(', ')
    feature_types = param_dict['feature_data'].split(', ')

    # make a new directory for output if needed
    write_dir = work_dir + 'results/'
    if not os.path.isdir(write_dir):
        os.mkdir(write_dir)

    # remove batch effect
    if batch == True:
        sc.pp.combat(traindata, key = 'batch', covariates = None, inplace = True)

    print('Setting up features for random forest classifier...')

    # create new AnnData object using all gene set features
    X_ad = ldmethods.make_features(0, available_features, traindata, feature_dict, feature_list)

    # re-rank feature list
    initial_ranking = ldmethods.compare_live_dead(X_ad)
    feature_list = list(initial_ranking.index)

    # count live and dead cells
    num_dead = X_ad.obs.dead.sum()
    num_dead_0 = num_dead
    num_live = len(X_ad.obs.dead) - num_dead
    num_live_0 = num_live
    pct_dead_0 = num_dead_0 / single_cells
    pct_live_0 = num_live_0 / single_cells
    print('Initially, there are {} dead cells and {} live cells'.format(num_dead, num_live))

    # calculate dead/live fold change and Wilcoxon p-values for each GO term
    df = ldmethods.compare_live_dead(X_ad, sort = False)
    df.sort_values(by = 'fdr_adj_pval', ascending = True).to_csv(write_dir + 'pvals_0.csv')

    # initialize columns in var and obs for tracking
    X_ad.var['last_p'] = df.fdr_adj_pval
    X_ad.var['last_fc'] = df.log2fc
    X_ad.var['votes'] = np.zeros(len(X_ad.var))
    X_ad.obs['n_switches'] = np.zeros(len(X_ad.obs))

    # df to fill during iterations
    rfc_tracker = pd.DataFrame(columns = ['round', 'prop_dead', 'n_votes', 'n_switches'])
    rfc_tracker.loc[0] = [0, num_dead / len(X_ad.obs), 0, 0]
    

    ###### TRAINING ITERATIVE RANDOM FOREST CLASSIFIER ######


    print('Starting iterations...')

    inner_iter = ceil(available_features / n_features)
    iter_count = 0

    # iteration of random forest classifier
    for i in range(1, rounds + 1):

        print('Round {}'.format(i))

        for j in tqdm(range(1, inner_iter + 1)):

            iter_count += 1

            # take jth subset of n_features
            imax = j * n_features
            imin = imax - n_features
            if imax > available_features:
                imax = available_features
            current_features = feature_list[imin:imax]
            sub_X_ad = X_ad[:, current_features]

            # fit random forest on subset and predict new labels
            rfc = RandomForestClassifier(class_weight = 'balanced_subsample', min_samples_split = rfc_minsplit, random_state = 0, n_jobs = n_jobs)
            rfc.fit(sub_X_ad.X, sub_X_ad.obs.dead)
            new_labels = rfc.predict(sub_X_ad.X)

            # check if the labels changed
            if X_ad.obs.dead.tolist() == list(new_labels):
                print('Labels are unchanged')
            else:
                new_switches = np.not_equal(new_labels, X_ad.obs.dead) * 1
                X_ad.obs.n_switches  = X_ad.obs.n_switches + new_switches
                X_ad.obs.dead = new_labels

            # compare live and dead cells
            num_dead = X_ad.obs.dead.sum()
            num_live = len(X_ad.obs.dead) - num_dead
            df = ldmethods.compare_live_dead(X_ad, sort = False)

            # compare p-values and fold changes, vote on features
            new_votes = (df.fdr_adj_pval < X_ad.var.last_p) | (np.absolute(df.log2fc) > np.absolute(X_ad.var.last_fc))
            X_ad.var.votes = X_ad.var.votes + (new_votes * 1)
            X_ad.var.last_p = df.fdr_adj_pval
            X_ad.var.last_fc = df.log2fc
            
            # save data from iteration
            pct_dead = num_dead / len(X_ad.obs)
            rfc_tracker.loc[iter_count] = [i, pct_dead, sum(new_votes), sum(new_switches)]


    ###### TESTING ITERATIVE RANDOM FOREST CLASSIFIER ######

    print('Done iterations, wrapping up...')

    # assign a dead label to cells that were relabeled many times
    cell_labels = X_ad.obs.dead
    cell_switches = X_ad.obs.n_switches
    threshold = ceil(rounds * 0.5)
    new_cell_labels = []
    for i, lab in enumerate(cell_labels):
        if cell_switches[i] >= threshold:
            new_cell_labels.append(True)
        else:
            new_cell_labels.append(lab)
    X_ad.obs.dead = new_cell_labels

    # remove features with 0 votes, sort features by votes, and select top 500 for testing
    ranked_features = X_ad.var.sort_values(by = ['votes', 'last_p'], ascending = [False, True])
    ranked_features = ranked_features[ranked_features.votes != 0]
    ranked_features.to_csv(write_dir + 'final_ranked_features.csv')
    ranked_features = ranked_features.index.tolist()
    if len(ranked_features) > 500:
        final_features = ranked_features[:500]
    else:
        final_features = ranked_features

    # use the final features to fit a random forest classifier
    final_train_ad = X_ad[:, final_features]
    rfc = RandomForestClassifier(class_weight = 'balanced_subsample', min_samples_split = rfc_minsplit, random_state = 0, n_jobs = n_jobs)
    rfc.fit(final_train_ad.X, final_train_ad.obs.dead)

    # get the sensitivity and specificity of the initial labels (based on mitochondrial genes)
    sensitivity, specificity, precision, accuracy, F1_score = ldmethods.score_classifier(testdata.obs.true_annotation, testdata.obs.dead)
    initial_test_labels = 'Before IRFC\n---------------------\nSensitivity:\t' + str(round(sensitivity, 4)) + '\nSpecificity:\t' + str(round(specificity, 4)) + '\nPrecision:\t' + str(round(precision, 4)) + '\nAccuracy:\t' + str(round(accuracy, 4)) + '\nF1 score:\t' + str(round(F1_score, 4))
    scores_df = pd.DataFrame(columns = ['sensitivity', 'specificity', 'precision', 'accuracy', 'F1_score'])
    scores_df.loc['traditional'] = [sensitivity, specificity, precision, accuracy, F1_score]

    # predict on testing set
    if batch == True:
        sc.pp.combat(testdata, key = 'batch', covariates = None, inplace = True)
    final_test_ad = ldmethods.make_features(0, len(final_features), testdata, feature_dict, final_features)
    final_test_ad.obs['predicted_dead'] = rfc.predict(final_test_ad.X)

    # evaluate performance
    score = rfc.score(final_test_ad.X, final_test_ad.obs.true_annotation)
    print(blue + 'The RFC score is {}'.format(score) + normal)
    sensitivity, specificity, precision, accuracy, F1_score = ldmethods.score_classifier(final_test_ad.obs.true_annotation, final_test_ad.obs.predicted_dead)
    scores_df.loc['IRFC'] = [sensitivity, specificity, precision, accuracy, F1_score]

    # plot umap of testing data
    ldmethods.plot_final_features(final_test_ad, write_dir + 'plot_umap.png', columns = ['dead', 'predicted_dead', 'true_annotation'])
    ldmethods.plot_classifier_scores(scores_df, write_dir)

    # save results
    test_results = '\n\n\n****** TESTING ******\n\n\n' + initial_test_labels + '\n\nAfter IRFC\n---------------------\nSensitivity:\t' + str(round(sensitivity, 4)) + '\nSpecificity:\t' + str(round(specificity, 4)) + '\nPrecision:\t' + str(round(precision, 4)) + '\nAccuracy:\t' + str(round(accuracy, 4)) + '\nF1 score:\t' + str(round(F1_score, 4)) + '\n\nRFC score:\t' + str(round(score, 4))


    ###### SAVE RESULTS ######


    # save summary info on all iterations
    rfc_tracker.to_csv(write_dir + 'iterations.csv')
    ldmethods.plot_iter_progress(range(len(rfc_tracker)), rfc_tracker.n_votes, '# new votes', write_dir + 'plot_n_votes')
    ldmethods.plot_iter_progress(range(len(rfc_tracker)), rfc_tracker.n_switches, '# switches', write_dir + 'plot_n_switches')
    ldmethods.plot_iter_progress(range(len(rfc_tracker)), rfc_tracker.prop_dead, 'proportion dead', write_dir + 'plot_proportion_dead')

    # save the last AnnData object
    # final_train_ad.write_h5ad(write_dir + 'selected_features.h5ad')

    # calculate and save similarity matrix between the remaining features
    cropped_features = set([ i.split('_')[0] for i in final_features ])
    selected_features_dict = { i: feature_dict[i] for i in cropped_features }
    sim_go, sim_mat = ldmethods.similarity(selected_features_dict)
    sim_df = pd.DataFrame(sim_mat, columns = sim_go, index = sim_go)
    ldmethods.plot_sim_matrix(sim_df, write_dir + 'plot_selected_features_similarity')
    sim_df.to_csv(write_dir + 'selected_features_similarity.csv')

    last_toc = time.process_time()
    runtime = last_toc - first_tic

    # generate run report
    run_report = 'RUN REPORT - IRFC 2\n\n\n****** TRACKING ******\n\n\nDatetime:\t' + now.strftime('%Y/%m/%d %H:%M') + ' UTC\nRuntime:\t' + str(round(runtime, 2)) + ' sec\nVersion:\t' + __version__ + '\n\nDirectory:\t' + write_dir + '\n\n\n****** DATA ******\n\n\nSingle cells:\t\t' + str(single_cells) + '\nInitial dead:\t\t' + str(num_dead_0) + ',\t' + str(round(pct_dead_0 * 100, 2)) + '%\nInitial live: \t\t' + str(num_live_0) + ',\t' + str(round(pct_live_0 * 100, 2)) + '%\n\nTotal features:\t\t' + str(available_features) + '\nFeature type:\t\t' + ', '.join(feature_types) + '\nData in features:\t' + ', '.join(feature_data) + '\n\n\n****** PARAMETERS ******\n\n\nSplit data:\t\t' + str(split_data) + '\nMax features in RFC:\t' + str(n_features) + '\nRFC minsplit:\t\t' + str(rfc_minsplit) + '\nRounds:\t\t\t' + str(rounds) + '\n\nThreads:\t\t' + str(n_jobs) + '\n\n\n****** RESULTS ******\n\n\nFinal dead:\t\t' + str(num_dead) + ',\t' + str(round(pct_dead * 100, 2)) + '%\nFinal feature count:\t' + str(len(final_features)) + test_results + '\n\n\nEOF'

    with open(write_dir + 'run_report.txt', "w") as text_file:
        text_file.write(run_report)

    print('Done')